---
SPDX-License-Identifier: MIT
path: "/tutorials/hyperconverged-proxmox-cloud"
slug: "hyperconverged-proxmox-cloud"
date: "2021-01-08"
title: "Setting up your own public cloud with Proxmox on Hetzner baremetal."
short_description: "Set up a resilient public cloud, with automatic failover, redundant storage and high speed networking."
tags: ["Development", "Proxmox", "Containers", "Docker", "Virtualization", "Cloud", "Failover"]
author: "Kenzi Marcel"
author_link: "https://github.com/kenzim"
author_img: "https://s.gravatar.com/avatar/aad2e58145359610f0bc11eec5412567?s=80&r=g"
author_description: "Logic is the beginning of wisdom, not the end."
language: "en"
available_languages: ["en"]
header_img: "header-3"
---

## Introduction

The concept of having hyperconverged infrastructure is that both your storage and compute resources reside on the same servers. Instead of having dedicated storage boxes, and then dedicated compute boxes, everything is stacked together on each server. Primarily, this greatly reduces cost, as you may only need half the number of servers, but it also increases performance, especially where you don't have high performance RDMA 100G network switches, as your virtual machines and other appliances can often look locally for storage instead of over the network.

In this tutorial, we will be setting up the three-node cluster, each with high performance NVMe storage drives, and Intel i9 9900k CPUs to ensure virtual machines are snappy and your applications are running as fast as they can. We will be comparing this to Digital Ocean, a well known and acclaimed cloud service provider. We will see how much money we can save for a comparable setup, and what the advantages are of your own servers.

## Price Breakdown

For your three servers to communicate properly, there will be additional network hardware we will have to order from Hetzner with our servers. This includes 10 Gigabit network cards for storage, and additional 1G cards for Proxmox's administrative communication. Below is the configuration I will use for one node in the cluster:

| Name                     | Quantity | Cost (EUR) | Subtotal (EUR) |
| ------------------------ | -------- | ---------- | -------------- |
| Base 9900k 128GB Server  | 1        | 69         | 69             |
| Dual 10G NIC (X710-DA2)  | 1        | 14         | 14             |
| 240G Datacentre Sata SSD | 2        | 5          | 10             |
| Lan Connection 10G       | 2        | 2.50       | 5              |
| 1G NIC                   | 1        | 2          | 2              |
| Lan Connection 1G        | 1        | 1          | 1              |
|                          |          | Total:     | €101.00        |

This is the cost for one node in the cluster. At a minimum, we will need three of these, bring the total for the servers to €307.50. This may seem like a lot, especially when other beefier servers can be had for less, but compared to other highly available services, this very affordable.

In addition to these servers, some network switches will be needed. For an optimal setup, the following is recommended:

| Name               | Quantity | Cost (EUR) | Subtotal (EUR) |
| ------------------ | -------- | ---------- | -------------- |
| 12 port 10G switch | 1        | 43         | 43             |
| 8 port 1G switch   | 12       | 2          | 2              |
|                    |          | Total:     | €45            |

For allowing servers to start with the same IP on any server, we will also need to purchase vSwitch IPs. These IPs can be used by any server at any time thats connected to the switch, as opposed to having IPs only usable on one server. This means your virtual machine can run on any node from the outside it would look the same. For 29 IPs, this will cost €35.29, which is probably the amount you'll need for a setup of this size. You can buy less if you know you are going to be using fewer, larger VMs instead of many smaller ones. Alternatively, if this is a private cloud, and you are using a meshed VPN like ZeroTier or Tinc, you won't need these external IPs.

[//]: #write about reserving rackspace

## Step 1 - Ordering the hardware

Ordering this custom setup is a bit more difficult that normal server ordering. First, when picking the EX-62 server, or comparable server that you are using for this setup, you must order the two extra 240gb Sata SSDs. Then, in the additional instructions, write that you would like all of the servers in the same rack, with space for two network switches. This means the setup will be done manually, but they have to setup the hardware anyway.

Then, to order the network hardware and switches, you will need to email Hetzner support. Write that you would like to order a 8 port 1 Gigabit switch and a 12 port 10 Gigabit switch with your recently purchased dedicated servers. You will also need to specify that you would like an Intel X710-DA2 and a 1G NIC with each server installed as well. Please write that you would like the 10G NICs connected to the 10G switch with two cables, and the Gigabit NIC into the Gigabit switch. This means that we will have one private gigabit network for Proxmox communication, and a 20 gigabit connection for storage communication.

## Step 2 - Installing Proxmox

The preferred way of installing Proxmox is by using the installation ISO. You can follow the tutorial [here](https://community.hetzner.com/tutorials/proxmox-docker-zfs) to install Proxmox without using a KVM switch. If you follow this guide, please install Proxmox onto the additional SSDs, and not the NVMe drives. You won't need to install docker either. If you are using the EX-62 servers, please note that the main NIC is called eno1 and doesn't use the predictable naming system for some reason with Proxmox.

## Step 3 - Configuring the Network

For everything to work smoothly, there are multiple network adapters we have to setup now. Please make sure that all the additional NICs have been ordered prior to starting this step.

### Step 3.1 - Creating vSwitch

To use shared IPs, we will need to setup a vSwitch. Fortunately, this is easily setup in the Robot panel. First, navigate to the vSwitch panel:

<img src="images/vswitch1.png" alt="Creating vSwitch" style="zoom:67%;" />

Then, create a vSwitch for your servers:

<img src="images/vswitch2.png" alt="Creating vSwitch" style="zoom:67%;" />

Next, we need to add our servers to the vSwitch:

<img src="images/vswitch3.png" alt="Creating vSwitch" style="zoom:67%;" />

Finally, we can order some external IP addresses for the vSwitch:

<img src="images/vswitch4.png" alt="Creating vSwitch" style="zoom:67%;" />

[//]: #assess if images are really needed

### Step 3.2 - Configuring Storage Network

The first network we will configure is the bonded 2*10 Gigabit connection for storage communications. This will give us a redundant and high speed connection for Ceph to replicate data between the nodes. It's essential that this connection is at least 10 Gigabit, otherwise performance will suffer greatly. Having a separate network from other traffic also helps latency.

```bash
auto bond0
iface bond0 inet static
      bond-slaves enp2s0f0 enp2s0f1
      address  10.254.253.10
      netmask  255.255.255.0
      bond-miimon 100
      bond-mode 802.3ad
      bond-xmit-hash-policy layer2+3
```

[//]: #find out the actual interface names for ex62 servers

[//]: #find out if the 10g switch supports link aggregation

### Step 3.2 - Configuring Corosync Network

Corosync is the Proxmox service that keeps things running in a cluster. Having services restart after failure in a timely manner is dependent on the latency between Corosync instances, so a private network dedicated to this traffic is ideal.

```bash
auto eno2
iface eno2 inet static
      address 10.254.254.10
      netmask 255.255.255.0
```

[//]: #find out the actual interface names for ex62 servers

### Step 3.3 - Configuring vSwitch Network

To utilise the vSwitch IPs, we will need to create a network for the VLan. Below is an example configuration:

```bash
iface eno1.4010 inet manual

auto vmbr1v4010
iface vmbr1v4010 inet manual
        bridge-ports eno1.4010
        bridge-stp off
        bridge-fd 0
```

Replace 4010 with the ID you chose when creating the vSwitch.

[//]: #test this configuration

### Step 3.4 - Example completed configuration

```bash
auto lo inet loopback

auto eno1
iface eno1 inet static
	address 1.1.1.20/24
	gateway 1.1.1.1
	pointopoint 1.1.1.1
	
iface eno1.4010 inet manual
	
auto eno2
iface eno2 inet static
    address 10.254.254.10
    netmask 255.255.255.0
	
auto bond0
iface bond0 inet static
    bond-slaves enp2s0f0 enp2s0f1
    address  10.254.253.10
    netmask  255.255.255.0
    bond-miimon 100
    bond-mode 802.3ad
    bond-xmit-hash-policy layer2+3
	
auto vmbr0
iface vmbr0 inet manual
	bridge-ports eno1
    bridge-stp off
    bridge-fd 0

auto vmbr1v4010
iface vmbr1v4010 inet manual
        bridge-ports eno1.4010
        bridge-stp off
        bridge-fd 0

auto vmbr2
iface vmbr2 inet static
    address 192.168.1.1/24
    bridge-ports none
    bridge-stp off
    bridge-fd 0

    post-up echo "1" > /proc/sys/net/ipv4/ip_forward
    post-up iptables -t nat -A POSTROUTING -s '192.168.1.0/24' -o eno1 -j MASQUERADE
    post-down iptables -t nat -D POSTROUTING -s '192.168.1.0/24' -o eno1 -j MASQUERADE
```

In this configuration, I have an extra bridge for VMs that aren't highly available. They can use `vmbr2` to use NAT and an internal IP to access the internet.

[//]: #test this config

[//]: #bonus zerotier vpn config ???

## Step 4 - Setting up Ceph storage

### Step 4.1 - Installing Ceph

### Step 4.2 - Creating Ceph Monitors, Masters and OSDs

### Step 4.3 - Adding Ceph to available Proxmox storages

## Step 5 - Configuring High Availability

### Step 5.1 - Creating a HA group

### Step 5.2 - Creating a HA enabled virtual machine

### Step 5.3 - Configuring VM Networking

## Comparison to similar solutions

### Benchmarking Digital Ocean

### Benchmarking our Proxmox setup

### Results

## Step 6 - Bonus features

### DHCP Server for Internal NAT Network

### Connecting to Hetzner Cloud 

### High availability on ZeroTier Meshed VPN

## Conclusion

Time to get that 99.9999% uptime money

##### License: MIT

<!--

Contributor's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: Kenzi Marcel - kenzi@kenzim.co.uk

-->
